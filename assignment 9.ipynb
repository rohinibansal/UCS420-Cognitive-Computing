{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ea085d-405d-448f-874d-cb60968c6db7",
   "metadata": {},
   "source": [
    "Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
    "technology, food, books, etc.).\n",
    "1. Convert text to lowercase and remove punctuaƟon.\n",
    "2. Tokenize the text into words and sentences.\n",
    "3. Remove stopwords (using NLTK's stopwords list).\n",
    "4. Display word frequency distribuƟon (excluding stopwords).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "081492d7-33aa-4bed-8828-4cec90ebb294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A short paragraph is a concise section of text, typically consisting of 100-200 words, that focuses on a single, specific topic. It's designed to be easily readable and understandable, often with a clear introduction, supporting details, and a concluding statement that reinforces the main idea. \n",
      "a short paragraph is a concise section of text, typically consisting of 100-200 words, that focuses on a single, specific topic. it's designed to be easily readable and understandable, often with a clear introduction, supporting details, and a concluding statement that reinforces the main idea. \n"
     ]
    }
   ],
   "source": [
    "para=\"\"\"A short paragraph is a concise section of text, typically consisting of 100-200 words, that focuses on a single, specific topic. It's designed to be easily readable and understandable, often with a clear introduction, supporting details, and a concluding statement that reinforces the main idea. \"\"\"\n",
    "print (para)\n",
    "para_lower=para.lower()\n",
    "print(para_lower)\n",
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfa07115-e409-4a73-b517-88cf274675cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'short', 'paragraph', 'is', 'a', 'concise', 'section', 'of', 'text', ',', 'typically', 'consisting', 'of', '100-200', 'words', ',', 'that', 'focuses', 'on', 'a', 'single', ',', 'specific', 'topic', '.', 'it', \"'s\", 'designed', 'to', 'be', 'easily', 'readable', 'and', 'understandable', ',', 'often', 'with', 'a', 'clear', 'introduction', ',', 'supporting', 'details', ',', 'and', 'a', 'concluding', 'statement', 'that', 'reinforces', 'the', 'main', 'idea', '.']\n",
      "['a short paragraph is a concise section of text, typically consisting of 100-200 words, that focuses on a single, specific topic.', \"it's designed to be easily readable and understandable, often with a clear introduction, supporting details, and a concluding statement that reinforces the main idea.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize,sent_tokenize\n",
    "words=word_tokenize(para_lower)\n",
    "print(words)\n",
    "sentences=sent_tokenize(para_lower)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73144d82-5c6a-4155-bf66-e77e516d31aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['short', 'paragraph', 'concise', 'section', 'text', ',', 'typically', 'consisting', '100-200', 'words', ',', 'focuses', 'single', ',', 'specific', 'topic', '.', \"'s\", 'designed', 'easily', 'readable', 'understandable', ',', 'often', 'clear', 'introduction', ',', 'supporting', 'details', ',', 'concluding', 'statement', 'reinforces', 'main', 'idea', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords=set(stopwords.words('english'))\n",
    "filtered_words=[word for word in words if word not in stopwords]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2ca11be-9840-4d93-88a1-416b5f297f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 6), ('.', 2), ('short', 1), ('paragraph', 1), ('concise', 1), ('section', 1), ('text', 1), ('typically', 1), ('consisting', 1), ('100-200', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "freq=FreqDist(filtered_words)\n",
    "print(freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f15a2-7daa-4c82-822e-321231adb5ae",
   "metadata": {},
   "source": [
    "Q2: Stemming and LemmaƟzaƟon\n",
    "1. Take the tokenized words from Question 1 (after stopword removal).\n",
    "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
    "3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
    "4. Compare and display results of both techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "029b7cf8-e1cf-4473-a578-f93c5d0e524f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['short', 'paragraph', 'concis', 'section', 'text', ',', 'typic', 'consist', '100-200', 'word', ',', 'focus', 'singl', ',', 'specif', 'topic', '.', \"'s\", 'design', 'easili', 'readabl', 'understand', ',', 'often', 'clear', 'introduct', ',', 'support', 'detail', ',', 'conclud', 'statement', 'reinforc', 'main', 'idea', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer,LancasterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "text=[stemmer.stem(word) for word in filtered_words]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf752a17-0e6f-4386-894e-8daaa22a4cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['short', 'paragraph', 'cont', 'sect', 'text', ',', 'typ', 'consist', '100-200', 'word', ',', 'focus', 'singl', ',', 'spec', 'top', '.', \"'s\", 'design', 'easy', 'read', 'understand', ',', 'oft', 'clear', 'introduc', ',', 'support', 'detail', ',', 'conclud', 'stat', 'reinforc', 'main', 'ide', '.']\n"
     ]
    }
   ],
   "source": [
    "s=LancasterStemmer()\n",
    "tt=[s.stem(word) for word in filtered_words]\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "caa25734-016d-4b9f-a71a-3f94ec415088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['short', 'paragraph', 'concise', 'section', 'text', ',', 'typically', 'consisting', '100-200', 'word', ',', 'focus', 'single', ',', 'specific', 'topic', '.', \"'s\", 'designed', 'easily', 'readable', 'understandable', ',', 'often', 'clear', 'introduction', ',', 'supporting', 'detail', ',', 'concluding', 'statement', 'reinforces', 'main', 'idea', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "l=WordNetLemmatizer()\n",
    "t=[l.lemmatize(word) for word in filtered_words]\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443da801-0722-4255-8618-695fce27f9b3",
   "metadata": {},
   "source": [
    "Q3. Regular Expressions and Text Spliƫng\n",
    "1. Take their original text from QuesƟon 1.\n",
    "2. Use regular expressions to:\n",
    "a. Extract all words with more than 5 leƩers.\n",
    "b. Extract all numbers (if any exist in their text).\n",
    "c. Extract all capitalized words.\n",
    "3. Use text spliƫng techniques to:\n",
    "a. Split the text into words containing only alphabets (removing digits and special\n",
    "characters).\n",
    "b. Extract words starƟng with a vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d752eef-d9dd-47aa-a87f-249d65ef1da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paragraph', 'concise', 'section', 'typically', 'consisting', 'focuses', 'single', 'specific', 'designed', 'easily', 'readable', 'understandable', 'introduction', 'supporting', 'details', 'concluding', 'statement', 'reinforces']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "words_more_than_5 = re.findall(r\"\\b\\w{6,}\\b\",para)\n",
    "print(words_more_than_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7baa7a5-fd9a-4c91-8c2e-be45bb39d86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', '200']\n"
     ]
    }
   ],
   "source": [
    "digits=re.findall(r\"\\d+\",para)\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b98eefc-47ba-4390-9b5d-9d31804f016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'It']\n"
     ]
    }
   ],
   "source": [
    "cap=re.findall(r\"\\b[A-Z][a-z]*\\b\",para)\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c927fab-9c6f-4df3-8fee-afeca1fdd75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'short', 'paragraph', 'is', 'a', 'concise', 'section', 'of', 'text', 'typically', 'consisting', 'of', 'words', 'that', 'focuses', 'on', 'a', 'single', 'specific', 'topic', 'It', 's', 'designed', 'to', 'be', 'easily', 'readable', 'and', 'understandable', 'often', 'with', 'a', 'clear', 'introduction', 'supporting', 'details', 'and', 'a', 'concluding', 'statement', 'that', 'reinforces', 'the', 'main', 'idea']\n"
     ]
    }
   ],
   "source": [
    "alpha=re.findall(r\"\\b[A-Za-z]+\\b\",para)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dad11885-cd30-4371-8c6c-d6b823f94790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'is', 'a', 'of', 'of', 'on', 'a', 'It', 'easily', 'and', 'understandable', 'often', 'a', 'introduction', 'and', 'a', 'idea']\n"
     ]
    }
   ],
   "source": [
    "vowel=[word for word in alpha if word[0].lower() in 'aeiou']\n",
    "print(vowel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d4b5b-e3ed-4c0e-b2f9-af6aba3ecc29",
   "metadata": {},
   "source": [
    "Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
    "1. Take original text from QuesƟon 1.\n",
    "2. Write a custom tokenizaƟon funcƟon that:\n",
    "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n",
    "\"isn't\" should not be split into \"is\" and \"n't\").\n",
    "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
    "a single token).\n",
    "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
    "should remain as is).\n",
    "\n",
    "3. Use Regex SubsƟtuƟons (re.sub) to:\n",
    "a. Replace email addresses with '<EMAIL>' placeholder.\n",
    "b. Replace URLs with '<URL>' placeholder.\n",
    "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
    "'<PHONE>' placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ffbdf92-98c0-41c4-8fd2-4c495f7dda72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'short', 'paragraph', 'is', 'a', 'concise', 'section', 'of', 'text', 'typically', 'consisting', 'of', '100-200', 'words', 'that', 'focuses', 'on', 'a', 'single', 'specific', 'topic', \"It's\", 'designed', 'to', 'be', 'easily', 'readable', 'and', 'understandable', 'often', 'with', 'a', 'clear', 'introduction', 'supporting', 'details', 'and', 'a', 'concluding', 'statement', 'that', 'reinforces', 'the', 'main', 'idea']\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(text):\n",
    "    pattern=r\"\\b\\w+(?:[-']\\w+)*\\b|\\d+\\.\\d+|\\d+\"\n",
    "    tokens=re.findall(pattern,text)\n",
    "    return tokens\n",
    "print(custom_tokenizer(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92b2a753-3d40-4b2d-a845-f726e17bdb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact me at <EMAIL> or visit <URL> My number is <PHONE> or <PHONE>. He isn't sure about the state-of-the-art design and the cost was 3.14 dollars.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\b[\\w.-]+?@[\\w.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text)\n",
    "    text = re.sub(r'(\\+?\\d{1,3}[\\s-]?)?\\d{3}[\\s-]?\\d{3}[\\s-]?\\d{4}', '<PHONE>', text)\n",
    "    return text\n",
    "text = \"Contact me at john.doe@example.com or visit https://example.com. My number is +91 9876543210 or 123-456-7890. He isn't sure about the state-of-the-art design and the cost was 3.14 dollars.\"\n",
    "print(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465ce8b-6199-44ee-88d7-5e96102b7f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
